{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from queue import LifoQueue\n",
    "import csv\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from itertools import combinations, product\n",
    "\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec, KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'line_profiler'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-df8a33df4eaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load_ext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'line_profiler'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\varvara_koshman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2305\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2306\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2307\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2308\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<c:\\users\\varvara_koshman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\decorator.py:decorator-gen-65>\u001b[0m in \u001b[0;36mload_ext\u001b[1;34m(self, module_str)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varvara_koshman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varvara_koshman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\magics\\extension.py\u001b[0m in \u001b[0;36mload_ext\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodule_str\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Missing module name.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextension_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'already loaded'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varvara_koshman\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\extensions.py\u001b[0m in \u001b[0;36mload_extension\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmodule_str\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mprepended_to_syspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                     \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                         print((\"Loading extensions from {dir} is deprecated. \"\n",
      "\u001b[1;32mc:\\users\\varvara_koshman\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varvara_koshman\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varvara_koshman\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varvara_koshman\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'line_profiler'"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    DATA_PATH = r'medicalTextTrees/parus_results'\n",
    "    files = os.listdir(DATA_PATH)\n",
    "    df_columns = ['id', 'form', 'lemma', 'upostag', 'xpostag', 'feats', 'head', 'deprel']\n",
    "    # trees_df = pd.DataFrame(columns=df_columns)\n",
    "    full_df = []\n",
    "    stable_df = []\n",
    "    many_roots_df = []\n",
    "    very_long_df = []\n",
    "    for file in files:\n",
    "        full_dir = os.path.join(DATA_PATH, file)\n",
    "        name = file.split('.')[0]\n",
    "        with open(full_dir, encoding='utf-8') as f:\n",
    "            this_df = pd.read_csv(f, sep='\\t', names=df_columns)\n",
    "            if this_df['id'].duplicated().any():\n",
    "                start_of_subtree_df = list(this_df.groupby(this_df.id).get_group(1).index)\n",
    "                boundaries = start_of_subtree_df + [max(list(this_df.index)) + 1]\n",
    "                list_of_dfs = [this_df.iloc[boundaries[n]:boundaries[n + 1]] for n in range(len(boundaries) - 1)]\n",
    "                local_counter = 1\n",
    "                for df in list_of_dfs:\n",
    "                    df['sent_name'] = name + '_' + str(local_counter)\n",
    "                    full_df.append(df)\n",
    "                    stable_df.append(df)\n",
    "                    # trees_df = pd.concat([trees_df, df], ignore_index=True)\n",
    "                    local_counter += 1\n",
    "            else:\n",
    "                this_df['sent_name'] = name\n",
    "                if this_df.groupby(this_df.deprel).get_group('ROOT').shape[0] > 1:\n",
    "                    many_roots_df.append(this_df)\n",
    "                elif this_df.shape[0] > 21:\n",
    "                    very_long_df.append(this_df)\n",
    "                else:\n",
    "                    stable_df.append(this_df)\n",
    "                # trees_df = pd.concat([trees_df, this_df], ignore_index=True)\n",
    "                full_df.append(this_df)\n",
    "    trees_df = pd.concat(stable_df, axis=0, ignore_index=True)\n",
    "    # delete useless data\n",
    "    # trees_df = trees_df.drop(columns=['upostag', 'xpostag', 'feats'], axis=1)\n",
    "    trees_df = trees_df.drop(columns=['xpostag', 'feats'], axis=1)\n",
    "    # trees_df.drop(index=[11067], inplace=True)\n",
    "    trees_df.loc[13742, 'deprel'] = 'разъяснит'\n",
    "\n",
    "    # delete relations of type PUNC and reindex\n",
    "    trees_df_filtered = trees_df[trees_df.deprel != 'PUNC']\n",
    "    # trees_df_filtered = trees_df_filtered.loc[trees_df_filtered['sent_name'] != '37918_12']\n",
    "    # trees_df_filtered = trees_df_filtered.loc[trees_df_filtered['sent_name'] != '38897_9'] # 1) very long sentence 2) 5 roots\n",
    "    trees_df_filtered = trees_df_filtered.reset_index(drop=True)\n",
    "    trees_df_filtered.index = trees_df_filtered.index + 1\n",
    "    # trees_df_filtered.loc[12239, 'deprel'] = '1-компл'\n",
    "    # trees_df_filtered.loc[12239, 'head'] = 2\n",
    "\n",
    "    trees_long_df = pd.concat(very_long_df, axis=0, ignore_index=True)\n",
    "    trees_roots_df = pd.concat(many_roots_df, axis=0, ignore_index=True)\n",
    "    trees_long_df = trees_long_df[trees_long_df.deprel != 'PUNC']\n",
    "    trees_roots_df = trees_roots_df[trees_roots_df.deprel != 'PUNC']\n",
    "    trees_long_df = trees_long_df.reset_index(drop=True)\n",
    "    trees_long_df.index = trees_long_df.index + 1\n",
    "    trees_roots_df = trees_roots_df.reset_index(drop=True)\n",
    "    trees_roots_df.index = trees_roots_df.index + 1\n",
    "\n",
    "    trees_full_df = pd.concat(full_df, axis=0, ignore_index=True)\n",
    "    trees_full_df = trees_full_df.reset_index(drop=True)\n",
    "    trees_full_df.index = trees_full_df.index + 1\n",
    "    trees_full_df.drop(columns=['upostag', 'xpostag', 'feats'], axis=1)\n",
    "    trees_full_df = trees_full_df[trees_full_df.deprel != 'PUNC']\n",
    "\n",
    "    replaced_numbers = [k for k, v in trees_full_df.lemma.str.contains('#').to_dict().items() if\n",
    "                        v == True]  # едленно, вставить выше\n",
    "    for num in replaced_numbers:\n",
    "        trees_df_filtered.loc[num, 'upostag'] = 'Num'\n",
    "        trees_full_df.loc[num, 'upostag'] = 'Num'\n",
    "\n",
    "    target_sents = list({'55338_41', '58401_7', '32384_8', '31736_14', '48714_8', '54996_6'}) # TEST\n",
    "    target_sents = list({'55338_41', '58401_7'})  # TEST\n",
    "    trees_df_filtered = trees_df_filtered.loc[trees_df_filtered.sent_name.isin(target_sents)] # TEST\n",
    "\n",
    "    # trees_full_df.loc[trees_full_df.index.isin(replaced_numbers)].assign(upostag = 'N')\n",
    "    return trees_full_df, trees_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(trees_df_filtered, lemmas, dict_lemmas_3, part_of_speech_node_id):\n",
    "    lemma_sent_df = trees_df_filtered[['lemma', 'sent_name']]\n",
    "    lemma_sent_dict = {}\n",
    "    for name, group in lemma_sent_df.groupby('sent_name'):\n",
    "        lemma_sent_dict[name] = []\n",
    "        for _, row in group.iterrows():\n",
    "            lemma_sent_dict[name].append(row['lemma'])\n",
    "    sentences = list(lemma_sent_dict.values())\n",
    "\n",
    "    only_medical = Word2Vec(sentences, min_count=1)\n",
    "    # X = only_medical[only_medical.wv.vocab]\n",
    "    # pca = PCA(n_components=2)\n",
    "    # result = pca.fit_transform(X)\n",
    "    # # create a scatter plot of the projection\n",
    "    # pyplot.scatter(result[:, 0], result[:, 1])\n",
    "    # words = list(only_medical.wv.vocab)\n",
    "    # for i, word in enumerate(words):\n",
    "    #     pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "\n",
    "    model_2 = Word2Vec(min_count=1)\n",
    "    model_2.build_vocab(sentences)\n",
    "    w2v_fpath = \"additionalCorpus/all_norm-sz100-w10-cb0-it1-min100.w2v\"\n",
    "\n",
    "    # model.intersect_word2vec_format\n",
    "    # similar_dict = {}\n",
    "    # for lemma in lemmas:\n",
    "    #     similar_dict[lemma] = model.most_similar(lemma)\n",
    "    # embeddings = [model[i] for i in lemmas]\n",
    "\n",
    "    # additional_model =\n",
    "    # w2v = Word2Vec.load_word2vec_format(w2v_fpath, binary=True, unicode_errors='ignore')\n",
    "    upper_bound = 50000\n",
    "    high_cosine_dist = 0.8\n",
    "    model = KeyedVectors.load_word2vec_format(w2v_fpath, binary=True, unicode_errors='ignore')\n",
    "    model_2.build_vocab([list(model.vocab.keys())[:upper_bound]], update=True)\n",
    "    model_2.intersect_word2vec_format(w2v_fpath, binary=True, lockf=1.0, unicode_errors='ignore')\n",
    "    model_2.train(sentences, total_examples=upper_bound, epochs=model_2.iter)\n",
    "    similar_dict = {lemma: model_2.most_similar(lemma, topn=15) for lemma in lemmas if not pattern.match(lemma)}\n",
    "    similar_lemmas_dict = {}\n",
    "    for lemma, similar_lemmas in similar_dict.items():\n",
    "        for similar_lemma, cosine_dist in similar_lemmas:\n",
    "            if cosine_dist > high_cosine_dist and similar_lemma in lemmas.keys() and part_of_speech_node_id[similar_lemma] == part_of_speech_node_id[lemma]:\n",
    "                if lemma not in similar_lemmas_dict.keys():\n",
    "                    similar_lemmas_dict[lemma] = [similar_lemma]\n",
    "                else:\n",
    "                    similar_lemmas_dict[lemma].append(similar_lemma)\n",
    "    all_values = [item for sublist in similar_lemmas_dict.values() for item in sublist]\n",
    "    most_freq = set([i for i in all_values if all_values.count(i) > 11])\n",
    "    similar_lemmas_dict_filtered = {}\n",
    "    for k, v in similar_lemmas_dict.items():\n",
    "        stable = set(v) - most_freq\n",
    "        if 0 < len(stable) <= 10:\n",
    "            similar_lemmas_dict_filtered[k] = stable\n",
    "\n",
    "    pprint.pprint(similar_lemmas_dict_filtered)\n",
    "    # similar_mapping = {}\n",
    "    for lemma, similar_lemmas in similar_lemmas_dict_filtered.items():\n",
    "        for similar_lemma in similar_lemmas:\n",
    "            lemmas[lemma].append(lemmas[similar_lemma][0])\n",
    "            # if lemmas[similar_lemma][0] not in similar_mapping.keys():\n",
    "            #     similar_mapping[lemmas[similar_lemma][0]] = [lemma]\n",
    "            # else:\n",
    "            #     similar_mapping[lemmas[similar_lemma][0]].append(lemma)\n",
    "    pprint.pprint(lemmas)\n",
    "\n",
    "    for lemma, similar_lemmas in similar_lemmas_dict_filtered.items():\n",
    "        for similar_lemma in similar_lemmas:\n",
    "            if lemma in dict_lemmas_3.keys():\n",
    "                dict_lemmas_3[lemma].append(lemmas[similar_lemma][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, id, lemma=None, form=None, sent_name=None, is_included=False):\n",
    "        self.id = id\n",
    "        self.lemma = lemma\n",
    "        self.form = form\n",
    "        self.sent_name = sent_name\n",
    "        self.is_included = is_included\n",
    "\n",
    "\n",
    "class Edge:\n",
    "    def __init__(self, node_id_from, node_id_to, weight):\n",
    "        self.node_from = node_id_from\n",
    "        self.node_to = node_id_to\n",
    "        self.weight = weight  # relation type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.edges = []\n",
    "        self.nodes = []\n",
    "        self.inactive = set()\n",
    "        self.created = set()\n",
    "        self.heights = {}\n",
    "        self.edges_dict_from = {}\n",
    "        self.edges_dict_to = {}\n",
    "        self.nodes_dict_id = {}\n",
    "        self.additional_nodes = set()\n",
    "        self.similar_lemmas = {}\n",
    "        self.global_similar_mapping = {}\n",
    "\n",
    "    def set_help_dict(self):\n",
    "        self.edges_dict_from = {k: list(v) for k, v in itertools.groupby(sorted(self.edges, key=lambda x: x.node_from),\n",
    "                                                                         key=lambda x: x.node_from)}\n",
    "        self.nodes_dict_id = {node.id: node for node in self.nodes}\n",
    "        self.edges_dict_to = {k: list(v) for k, v in itertools.groupby(self.edges, key=lambda x: x.node_to)}\n",
    "        # filtered_edges = list(filter(lambda x: x.node_to not in self.additional_nodes and x.node_from not in self.additional_nodes, self.edges))\n",
    "        # self.edges_dict_to = {k: list(set(v) - self.additional_nodes) for k, v in itertools.groupby(filtered_edges, key=lambda x: x.node_to)}\n",
    "\n",
    "    def add_node(self, node):\n",
    "        self.nodes.append(node)\n",
    "\n",
    "    def add_edge(self, edge):\n",
    "        self.edges.append(edge)\n",
    "\n",
    "    def get_node(self, node_id):\n",
    "        return self.nodes_dict_id.get(node_id)  # return Node class instance\n",
    "\n",
    "    def get_children(self, node_id):\n",
    "        edges = self.edges_dict_from.get(node_id)\n",
    "        # only_active = list(filter(lambda edge: edge.node_to not in self.inactive and edge.node_from not in self.inactive, edges if edges is not None else []))\n",
    "        # return list(map(lambda x: x.node_to, only_active if only_active is not None else []))\n",
    "        return set(map(lambda x: x.node_to, edges if edges is not None else []))\n",
    "\n",
    "    def get_edge(self, to_id):\n",
    "        return self.edges_dict_to.get(to_id)\n",
    "\n",
    "    def remove_edge(self, to_id):\n",
    "        self.edges = list(filter(lambda x: x.node_to != to_id, self.edges))\n",
    "\n",
    "    def copy_node_details(self, existing_node):\n",
    "        new_node = Node(id=len(self.nodes),\n",
    "                    form=existing_node.form,\n",
    "                    sent_name=existing_node.sent_name,\n",
    "                    is_included=existing_node.is_included)\n",
    "        self.nodes_dict_id[new_node.id] = new_node\n",
    "        self.created.add(new_node.id)\n",
    "        return new_node\n",
    "\n",
    "    def add_new_edges(self, new_node_id, children):\n",
    "        for child_id in children:\n",
    "            new_edge = Edge(new_node_id, child_id, self.get_edge(child_id)[0].weight)\n",
    "            if child_id in self.edges_dict_to.keys():\n",
    "                self.edges_dict_to[child_id].append(new_edge)\n",
    "            else:\n",
    "                self.edges_dict_to[child_id] = [new_edge]\n",
    "            self.edges.append(new_edge)\n",
    "            if new_node_id in self.edges_dict_from.keys():\n",
    "                self.edges_dict_from[new_node_id].append(new_edge)\n",
    "            else:\n",
    "                self.edges_dict_from[new_node_id] = [new_edge]\n",
    "\n",
    "    def add_edge_to_dict(self, edge):\n",
    "        self.edges_dict_to[edge.node_to] = [edge]\n",
    "        if edge.node_from in self.edges_dict_from.keys():\n",
    "            self.edges_dict_from[edge.node_from].append(edge)\n",
    "        else:\n",
    "            self.edges_dict_from[edge.node_from] = edge\n",
    "        self.edges.append(edge)\n",
    "\n",
    "    def add_node_to_dict(self, node):\n",
    "        self.nodes_dict_id[node.id] = node\n",
    "        self.nodes.append(node)\n",
    "\n",
    "    def add_inactive(self, node_id):\n",
    "        self.inactive.add(node_id)\n",
    "\n",
    "    def get_children_nodes(self, children):\n",
    "        children_nodes = {\n",
    "            str(Tree.get_edge(self, child)[0].weight) + str(Tree.get_node(self, child).lemma): child for\n",
    "            child in children}\n",
    "        return children_nodes\n",
    "    \n",
    "    def calculate_heights(self):\n",
    "        # visited = np.full(len(self.nodes), False, dtype=bool)\n",
    "        visited = {node.id: False for node in self.nodes}\n",
    "        # self.heights = np.full(len(self.nodes), -1, dtype=int)  # all heights are -1 initially\n",
    "        stack = LifoQueue()\n",
    "        stack.put(0)\n",
    "        prev = None\n",
    "        while stack.qsize() > 0:\n",
    "            curr = stack.get()\n",
    "            stack.put(curr)\n",
    "            if not visited[curr]:\n",
    "                visited[curr] = True\n",
    "            children = self.get_children(curr)\n",
    "            if len(children) == 0:\n",
    "                self.heights[curr] = [0]\n",
    "                prev = curr\n",
    "                stack.get()\n",
    "            else:\n",
    "                all_visited_flag = True\n",
    "                children_filtered = set(children) - self.additional_nodes\n",
    "                for child in children_filtered:\n",
    "                    try:\n",
    "                        if not visited[child]:\n",
    "                            all_visited_flag = False\n",
    "                            stack.put(child)\n",
    "                    except IndexError as ie:\n",
    "                        ff = []\n",
    "                if all_visited_flag:\n",
    "                    curr_height = []\n",
    "                    if len(children_filtered) > 1:\n",
    "                        for child in children_filtered:\n",
    "                            for child_height in self.heights[child]:\n",
    "                                curr_height.append(child_height + 1)\n",
    "                    else:\n",
    "                        try:\n",
    "                            curr_height = [h + 1 for h in self.heights[prev]]\n",
    "                        except KeyError as e:\n",
    "                            efl = []\n",
    "                    self.heights[curr] = list(set(curr_height))\n",
    "                    prev = curr\n",
    "                    stack.get()\n",
    "    def simple_dfs(self, vertex, subtree_vertices):\n",
    "        sequence = []\n",
    "        node = self.get_node(vertex)\n",
    "        if node is not None:\n",
    "            sequence.append(tuple([vertex, node.lemma, node.form, node.sent_name]))\n",
    "            # visited = np.full(len(self.nodes), False, dtype=bool)\n",
    "            visited = {node.id: False for node in self.nodes}\n",
    "            stack = [vertex]\n",
    "            while len(stack) > 0:\n",
    "                curr = stack[-1]\n",
    "                if not visited[curr]:\n",
    "                    visited[curr] = True\n",
    "                children = self.get_children(curr)\n",
    "                if len(children) == 0:\n",
    "                    stack.pop()\n",
    "                else:\n",
    "                    all_visited_flag = True\n",
    "                    for child in children:\n",
    "                        if child in subtree_vertices and not visited[child]:\n",
    "                            all_visited_flag = False\n",
    "                            stack.append(child)\n",
    "                            node = self.get_node(child)\n",
    "                            sequence.append(tuple([child, node.lemma, node.form, node.sent_name]))\n",
    "                    if all_visited_flag:\n",
    "                        stack.pop()\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tree(trees_df_filtered, dict_lemmas, dict_rel, dict_lemmas_rev):\n",
    "    # construct a tree with a list of edges and a list of nodes\n",
    "    whole_tree = Tree()\n",
    "    root_node = Node(0, 0)  # add root\n",
    "    Tree.add_node(whole_tree, root_node)\n",
    "    # new_id_count = len(trees_df_filtered) + 1\n",
    "    new_id_count = 1\n",
    "    similar_lemmas_dict = {}\n",
    "    global_similar_mapping = {}\n",
    "    for name, group in trees_df_filtered.groupby('sent_name'):\n",
    "        row_ids = trees_df_filtered.index[trees_df_filtered.sent_name == name].to_list()\n",
    "        # temporary dictionary for remapping indices\n",
    "        temp_dict = {key: row_ids[ind] for ind, key in enumerate(group.id.to_list())}\n",
    "        temp_dict[0] = 0\n",
    "        children_dict = {}\n",
    "        edge_to_weight = {}\n",
    "        for _, row in group.iterrows():\n",
    "            # parameters for main node\n",
    "            curr_lemmas = dict_lemmas.get(row['lemma'])\n",
    "            main_id = temp_dict.get(row['id'])\n",
    "            from_id = temp_dict.get(row['head'])\n",
    "            weight = dict_rel.get(row['deprel'])\n",
    "            sent = row['sent_name']\n",
    "            form = row['form']\n",
    "            # create main node\n",
    "            create_new_node(whole_tree, main_id, curr_lemmas[0], form, sent, weight, from_id)\n",
    "            children = [main_id]\n",
    "            edge_to_weight[main_id] = weight\n",
    "            global_similar_mapping[main_id] = main_id\n",
    "            # if lemma has additional values add additional nodes\n",
    "            if len(curr_lemmas) > 1:\n",
    "                for i in range(1, len(curr_lemmas)):\n",
    "                    new_id_count += 1\n",
    "                    while new_id_count in list(map(lambda x: x.id, whole_tree.nodes)):\n",
    "                        new_id_count += 1\n",
    "                    create_new_node(whole_tree, new_id_count, curr_lemmas[i], form, sent, weight, from_id)\n",
    "                    edge_to_weight[new_id_count] = weight\n",
    "                    if main_id not in similar_lemmas_dict.keys():\n",
    "                        similar_lemmas_dict[main_id] = [new_id_count]\n",
    "                    else:\n",
    "                        similar_lemmas_dict[main_id].append(new_id_count)\n",
    "                    global_similar_mapping[new_id_count] = main_id\n",
    "                    children.append(new_id_count)\n",
    "            if from_id not in children_dict.keys():\n",
    "                children_dict[from_id] = children\n",
    "            else:\n",
    "                children_dict[from_id].extend(children)\n",
    "        # if parent has additional values add additional edges\n",
    "        for from_id, children in children_dict.items():\n",
    "            if from_id in similar_lemmas_dict.keys():\n",
    "                similar_ids = similar_lemmas_dict[from_id]\n",
    "                for similar_id in similar_ids:\n",
    "                    for child_id in children:\n",
    "                        Tree.add_edge(whole_tree, Edge(similar_id, child_id, edge_to_weight[child_id]))\n",
    "        #\n",
    "        # for main_id, similar_ids in similar_lemmas_dict.items():\n",
    "        #     if main_id in children_dict.keys():\n",
    "        #         for child in children_dict[main_id]:\n",
    "        #             for similar_id in similar_ids:\n",
    "        #                 Tree.add_edge(whole_tree, Edge(similar_id, child, edge_to_weight[main_id]))\n",
    "    whole_tree.additional_nodes = set([sublist for list in similar_lemmas_dict.values() for sublist in list])\n",
    "    whole_tree.similar_lemmas = similar_lemmas_dict\n",
    "    whole_tree.global_similar_mapping = global_similar_mapping\n",
    "    return whole_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_children_to_parents(k_2, filtered_groups, whole_tree, curr_height, old_node_new_nodes):\n",
    "    all_parents = set()\n",
    "    for k, v in filtered_groups.items():\n",
    "        for v_id in list(v):\n",
    "            edge_to_curr = Tree.get_edge(whole_tree, v_id)\n",
    "            Tree.get_node(whole_tree, v_id).is_included = True\n",
    "            if edge_to_curr is not None:\n",
    "                parent = edge_to_curr[0].node_from\n",
    "                try:\n",
    "                    if parent not in whole_tree.heights.keys(): # same as in whole_tree.additional\n",
    "                        parent = whole_tree.global_similar_mapping[parent]\n",
    "                    if max(whole_tree.heights[parent]) > curr_height:\n",
    "                        all_parents.add(parent)\n",
    "                except KeyError as ke:\n",
    "                    dfkdf = []\n",
    "                if v_id in old_node_new_nodes.keys():\n",
    "                    lemmas_to_visit = old_node_new_nodes[v_id]\n",
    "                else:\n",
    "                    lemmas_to_visit = [k]\n",
    "                if v_id in whole_tree.similar_lemmas.keys():\n",
    "                    for node_id in whole_tree.similar_lemmas[v_id]:\n",
    "                        lemmas_to_visit.append(Tree.get_node(whole_tree, node_id).lemma)\n",
    "                for lemma in lemmas_to_visit:\n",
    "                    label_for_child = str(edge_to_curr[0].weight) + str(lemma)\n",
    "                    if parent not in k_2.keys():\n",
    "                        k_2[parent] = {label_for_child}\n",
    "                    else:\n",
    "                        k_2[parent].add(label_for_child)\n",
    "    return all_parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_additional_children_to_parents(k_2, whole_tree, all_parents):\n",
    "    additional_child_nodes = {}\n",
    "    for parent in all_parents:\n",
    "        for child_id in Tree.get_children(whole_tree, parent):# - whole_tree.additional_nodes:\n",
    "            edge_to_curr = Tree.get_edge(whole_tree, child_id)[0]\n",
    "            child_node = Tree.get_node(whole_tree, child_id)\n",
    "            if not child_node.is_included:\n",
    "                label_for_child = str(edge_to_curr.weight) + str(child_node.lemma)\n",
    "                k_2[parent].add(label_for_child)\n",
    "                child_node.is_included = True\n",
    "                if parent not in additional_child_nodes.keys():\n",
    "                    additional_child_nodes[label_for_child] = child_id\n",
    "                else:\n",
    "                    additional_child_nodes[label_for_child].append(child_id)\n",
    "    return additional_child_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_combinations(k_2, v_id, str_sequence_help, str_sequence_help_reversed, equal_nodes, equal_nodes_mapping):\n",
    "    if len(equal_nodes) > 0:\n",
    "        list_for_combinations = []\n",
    "        prepared_k_2 = set()\n",
    "        included_labels = []\n",
    "        for child_tree in k_2[v_id]:\n",
    "            if child_tree in [item for sublist in list(equal_nodes.values()) for item in\n",
    "                              sublist] or child_tree in equal_nodes.keys():\n",
    "                if child_tree in equal_nodes_mapping.keys():\n",
    "                    actual_label = equal_nodes_mapping[child_tree]\n",
    "                else:\n",
    "                    actual_label = child_tree\n",
    "                if actual_label not in included_labels:\n",
    "                    if actual_label in equal_nodes.keys():\n",
    "                        list_for_combinations.append(equal_nodes[actual_label])\n",
    "                    else:\n",
    "                        list_for_combinations.append(equal_nodes[child_tree])\n",
    "                    included_labels.append(actual_label)\n",
    "            else:\n",
    "                prepared_k_2.add(child_tree)\n",
    "        combinations_repeated = list(product(*(list_for_combinations)))\n",
    "        all_combinations = []\n",
    "        if len(prepared_k_2) > 0:\n",
    "            for l in combinations_repeated:\n",
    "                merged = list(l) + list(prepared_k_2)\n",
    "                all_combinations.extend(list(combinations(merged, i)) for i in range(1, len(merged) + 1))\n",
    "        else:\n",
    "            for l in combinations_repeated:\n",
    "                all_combinations.extend(list(combinations(list(l), i)) for i in range(1, len(list(l)) + 1))\n",
    "    else:\n",
    "        list_for_combinations = k_2[v_id]\n",
    "        all_combinations = [list(combinations(list_for_combinations, i)) for i in\n",
    "                            range(1, len(list_for_combinations) + 1)]\n",
    "    all_combinations_str_joined = set()\n",
    "    for comb in all_combinations:\n",
    "        for tup in comb:\n",
    "            combs = [str(item) for item in sorted(list(tup))]\n",
    "            joined_label = EMPTY_STR.join(combs)\n",
    "            if joined_label not in str_sequence_help.keys():\n",
    "                str_sequence_help[joined_label] = combs\n",
    "                str_sequence_help_reversed[tuple(combs)] = joined_label\n",
    "            all_combinations_str_joined.add(joined_label)\n",
    "    return all_combinations_str_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodeid_repeats(filtered_combination_ids, str_sequence_help):\n",
    "    dict_nodeid_comb = {}\n",
    "    for k, v in filtered_combination_ids.items():\n",
    "        for v_i in v:\n",
    "            if v_i in dict_nodeid_comb.keys():\n",
    "                dict_nodeid_comb[v_i].append(str_sequence_help.get(k))\n",
    "            else:\n",
    "                dict_nodeid_comb[v_i] = [str_sequence_help.get(k)]\n",
    "    return dict_nodeid_comb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_part_new_new(whole_tree, lemma_count, grouped_heights):\n",
    "    classes_subtreeid_nodes = {}\n",
    "    classes_subtreeid_nodes_list = {}\n",
    "    unique_subtrees_mapped_global_subtree_lemma = {}\n",
    "    old_node_new_nodes = {}\n",
    "    equal_nodes_mapping = {}\n",
    "    subtree_label_sent = {}\n",
    "    k_2 = {}  # identifiers of edges of subtrees\n",
    "    lemma_nodeid_dict = {}\n",
    "    saved_combinations = []\n",
    "    for nodes in grouped_heights:\n",
    "        curr_height = nodes[0]\n",
    "        print(curr_height)\n",
    "        start = time.time()\n",
    "        id_lemma_dict = {node.id: node.lemma for node in nodes[1]}\n",
    "        grouped_lemmas = defaultdict(list)\n",
    "        for key, value in id_lemma_dict.items():\n",
    "            grouped_lemmas[value].append(key)\n",
    "        all_parents = add_children_to_parents(k_2, grouped_lemmas, whole_tree, curr_height, old_node_new_nodes)\n",
    "        additional_child_nodes = add_additional_children_to_parents(k_2, whole_tree, all_parents)\n",
    "        for additional_child, child_id in additional_child_nodes.items():\n",
    "            if additional_child not in lemma_nodeid_dict.keys():\n",
    "                lemma_nodeid_dict[additional_child] = {child_id}\n",
    "            else:\n",
    "                lemma_nodeid_dict[additional_child].add(child_id)\n",
    "        for lemma, ids in grouped_lemmas.items():\n",
    "            for v_id in ids:\n",
    "                edge_to_curr = Tree.get_edge(whole_tree, v_id)\n",
    "                if edge_to_curr is not None:\n",
    "                    label_for_child = str(edge_to_curr[0].weight) + str(lemma)\n",
    "                    if label_for_child not in lemma_nodeid_dict.keys():\n",
    "                        lemma_nodeid_dict[label_for_child] = {v_id}\n",
    "                    else:\n",
    "                        lemma_nodeid_dict[label_for_child].add(v_id)\n",
    "\n",
    "        if curr_height != 0:  # not applicable to leaves, leaves don't have subtrees\n",
    "            filtered_groups = {k: v for k, v in grouped_lemmas.items() if len(v) > 1}\n",
    "            for lemma, ids in filtered_groups.items():\n",
    "                combination_ids = {}\n",
    "                str_sequence_help = {}\n",
    "                str_sequence_help_reversed = {}\n",
    "\n",
    "                # generate combinations\n",
    "                for v_id in ids:\n",
    "                    equal_nodes = {}\n",
    "                    # only for duplicating nodes\n",
    "                    children = Tree.get_children(whole_tree, v_id) - whole_tree.created - whole_tree.additional_nodes\n",
    "                    for child in children:\n",
    "                        if child in old_node_new_nodes.keys() or child in whole_tree.similar_lemmas.keys():\n",
    "                            edge_to_child = Tree.get_edge(whole_tree, child)[0]\n",
    "                            child_node = Tree.get_node(whole_tree, child)\n",
    "                            w = str(edge_to_child.weight)\n",
    "                            actual_label = w + str(child_node.lemma)\n",
    "                            merge = []\n",
    "                            if child in old_node_new_nodes.keys():\n",
    "                                for l in old_node_new_nodes[child]: # refactor this and below\n",
    "                                    new_label = w + str(l)\n",
    "                                    merge.append(new_label)\n",
    "                                    equal_nodes_mapping[new_label] = actual_label\n",
    "                            else:\n",
    "                                merge.append(actual_label)\n",
    "                                for node_id in whole_tree.similar_lemmas[child]:\n",
    "                                    new_label = w + str(Tree.get_node(whole_tree, node_id).lemma)\n",
    "                                    merge.append(new_label)\n",
    "                                    equal_nodes_mapping[new_label] = actual_label\n",
    "                            if actual_label not in equal_nodes.keys():\n",
    "                                equal_nodes[actual_label] = merge\n",
    "                            else:\n",
    "                                equal_nodes[actual_label].extend(merge)\n",
    "                    all_combinations_str_joined = produce_combinations(k_2, v_id, str_sequence_help,\n",
    "                                                                       str_sequence_help_reversed, equal_nodes,\n",
    "                                                                       equal_nodes_mapping)\n",
    "                    for label in all_combinations_str_joined:\n",
    "                        if label in combination_ids.keys():\n",
    "                            combination_ids[label].append(v_id)\n",
    "                        else:\n",
    "                            combination_ids[label] = [v_id]\n",
    "\n",
    "                filtered_combination_ids = {k: v for k, v in combination_ids.items() if len(v) > 1}\n",
    "                for tree_label, node_list in filtered_combination_ids.items():\n",
    "                    str_tree_label = str(lemma) + tree_label\n",
    "                    if str_tree_label not in unique_subtrees_mapped_global_subtree_lemma.keys():\n",
    "                        unique_subtrees_mapped_global_subtree_lemma[str_tree_label] = lemma_count\n",
    "                        lemma_count += 1\n",
    "                # 16: [['107', '919'], ['208', '919'], ['919'], ['107'], ['208']]\n",
    "                dict_nodeid_comb = get_nodeid_repeats(filtered_combination_ids, str_sequence_help)\n",
    "                for node_id, node_subtrees in dict_nodeid_comb.items():\n",
    "                    existing_node = Tree.get_node(whole_tree, node_id)\n",
    "                    edge_to_curr = Tree.get_edge(whole_tree, node_id)[0]\n",
    "                    children = Tree.get_children(whole_tree, node_id)\n",
    "                    for subtree in node_subtrees:\n",
    "                        subtree_text = str_sequence_help_reversed.get(tuple(subtree))\n",
    "                        subtree_new_label = unique_subtrees_mapped_global_subtree_lemma.get(str(lemma) + subtree_text)\n",
    "\n",
    "                        if subtree_new_label not in subtree_label_sent.keys() or existing_node.sent_name not in subtree_label_sent[subtree_new_label]:\n",
    "                            # add new node with a new lemma\n",
    "                            new_node = Tree.copy_node_details(whole_tree, existing_node)\n",
    "                            new_node.lemma = subtree_new_label\n",
    "                            Tree.add_node_to_dict(whole_tree, new_node)\n",
    "\n",
    "                            if subtree_new_label not in subtree_label_sent.keys():\n",
    "                                subtree_label_sent[subtree_new_label] = [existing_node.sent_name]\n",
    "                            else:\n",
    "                                subtree_label_sent[subtree_new_label].append(existing_node.sent_name)\n",
    "\n",
    "                            # add new node to node aliases\n",
    "                            if node_id not in old_node_new_nodes.keys():\n",
    "                                old_node_new_nodes[node_id] = [new_node.lemma]\n",
    "                            else:\n",
    "                                old_node_new_nodes[node_id].append(new_node.lemma)\n",
    "\n",
    "                            # add an edge to it\n",
    "                            edge = Edge(edge_to_curr.node_from, new_node.id, edge_to_curr.weight)\n",
    "                            Tree.add_edge_to_dict(whole_tree, edge)\n",
    "\n",
    "                            # add new node to parent\n",
    "                            parent_subtree_text = str(edge_to_curr.weight) + str(subtree_new_label)\n",
    "                            if parent_subtree_text not in lemma_nodeid_dict.keys():\n",
    "                                lemma_nodeid_dict[parent_subtree_text] = {new_node.id}\n",
    "                            else:\n",
    "                                lemma_nodeid_dict[parent_subtree_text].add(new_node.id)\n",
    "\n",
    "                            subtree_children = []\n",
    "                            children_nodes = {}\n",
    "                            for subtree_node in subtree:\n",
    "                                if subtree_node not in lemma_nodeid_dict.keys():\n",
    "                                    if subtree_node in equal_nodes_mapping.keys():\n",
    "                                        target_child = \\\n",
    "                                        list(set(lemma_nodeid_dict[equal_nodes_mapping[subtree_node]]) & children)[0]\n",
    "                                    else:\n",
    "                                        if len(children_nodes) == 0:\n",
    "                                            children_nodes = Tree.get_children_nodes(whole_tree, children)\n",
    "                                        target_child = children_nodes[subtree_node]\n",
    "                                else:\n",
    "                                    intersection = set(lemma_nodeid_dict[subtree_node]) & children\n",
    "                                    if len(intersection) == 0:\n",
    "                                        if len(children_nodes) == 0:\n",
    "                                            children_nodes = Tree.get_children_nodes(whole_tree, children)\n",
    "                                        try:\n",
    "                                            target_child = children_nodes[subtree_node]\n",
    "                                        except KeyError as ke:\n",
    "                                            gh = []\n",
    "                                    else:\n",
    "                                        target_child = list(intersection)[0]\n",
    "                                subtree_children.append(target_child)\n",
    "\n",
    "                            if len(subtree_children) > 0:\n",
    "                                general_comb = ''.join(sorted([str(whole_tree.global_similar_mapping[child_id]) for child_id in subtree_children]))\n",
    "                                if general_comb not in saved_combinations:\n",
    "                                    # add edges to subtree's children from new node\n",
    "                                    Tree.add_new_edges(whole_tree, new_node.id, subtree_children)\n",
    "\n",
    "                                    # assign class\n",
    "                                    if subtree_new_label not in classes_subtreeid_nodes.keys():\n",
    "                                        classes_subtreeid_nodes[subtree_new_label] = [new_node.id]\n",
    "                                    else:\n",
    "                                        classes_subtreeid_nodes[subtree_new_label].append(new_node.id)\n",
    "\n",
    "                                    subtree_deep_children = set()\n",
    "                                    for subtree_lemma in list(\n",
    "                                            map(lambda x: Tree.get_node(whole_tree, x).lemma, subtree_children)):\n",
    "                                        if subtree_lemma in classes_subtreeid_nodes_list.keys():\n",
    "                                            subtree_deep_children.update(classes_subtreeid_nodes_list[subtree_lemma])\n",
    "                                    subtree_deep_children.update(subtree_children)\n",
    "                                    only_active = subtree_deep_children - whole_tree.inactive\n",
    "                                    # only_active = (whole_tree.inactive.symmetric_difference(set_children))&set_children\n",
    "\n",
    "                                    if subtree_new_label not in classes_subtreeid_nodes_list.keys():\n",
    "                                        classes_subtreeid_nodes_list[subtree_new_label] = only_active\n",
    "                                    else:\n",
    "                                        classes_subtreeid_nodes_list[subtree_new_label].update(only_active)\n",
    "                                    classes_subtreeid_nodes_list[subtree_new_label].add(new_node.id)\n",
    "\n",
    "                                    saved_combinations.append(general_comb)\n",
    "\n",
    "                    # remove old node and edges to/from it\n",
    "                    Tree.add_inactive(whole_tree, node_id)\n",
    "        print(time.time() - start)\n",
    "    classes_subtreeid_nodes = {k: v for k, v in classes_subtreeid_nodes.items() if len(v) > 1} # TODO: why do len=1 entries even appear here??\n",
    "    return classes_subtreeid_nodes, classes_subtreeid_nodes_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-165-40daba35bc3a>:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sent_name'] = name + '_' + str(local_counter)\n"
     ]
    }
   ],
   "source": [
    "%lprun -f read_data read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-156-0571692d4150>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sent_name'] = name + '_' + str(local_counter)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>form</th>\n",
       "      <th>lemma</th>\n",
       "      <th>head</th>\n",
       "      <th>deprel</th>\n",
       "      <th>sent_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Около</td>\n",
       "      <td>около</td>\n",
       "      <td>4</td>\n",
       "      <td>предл</td>\n",
       "      <td>52123_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>#число</td>\n",
       "      <td>3</td>\n",
       "      <td>количест</td>\n",
       "      <td>52123_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>лет</td>\n",
       "      <td>год</td>\n",
       "      <td>1</td>\n",
       "      <td>предл</td>\n",
       "      <td>52123_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>назад</td>\n",
       "      <td>назад</td>\n",
       "      <td>5</td>\n",
       "      <td>обст</td>\n",
       "      <td>52123_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>ЧМТ</td>\n",
       "      <td>ЧМТ</td>\n",
       "      <td>0</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>52123_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15255</th>\n",
       "      <td>14</td>\n",
       "      <td>в</td>\n",
       "      <td>в</td>\n",
       "      <td>17</td>\n",
       "      <td>обст</td>\n",
       "      <td>34695_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15256</th>\n",
       "      <td>15</td>\n",
       "      <td>армии</td>\n",
       "      <td>армия</td>\n",
       "      <td>14</td>\n",
       "      <td>предл</td>\n",
       "      <td>34695_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15257</th>\n",
       "      <td>16</td>\n",
       "      <td>не</td>\n",
       "      <td>не</td>\n",
       "      <td>17</td>\n",
       "      <td>огранич</td>\n",
       "      <td>34695_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15258</th>\n",
       "      <td>17</td>\n",
       "      <td>служил</td>\n",
       "      <td>служить</td>\n",
       "      <td>1</td>\n",
       "      <td>сочин</td>\n",
       "      <td>34695_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15259</th>\n",
       "      <td>19</td>\n",
       "      <td>ЧМТ</td>\n",
       "      <td>чмт</td>\n",
       "      <td>17</td>\n",
       "      <td>примыкат</td>\n",
       "      <td>34695_0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15259 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    form    lemma  head    deprel sent_name\n",
       "1       1   Около    около     4     предл   52123_9\n",
       "2       2      10   #число     3  количест   52123_9\n",
       "3       3     лет      год     1     предл   52123_9\n",
       "4       4   назад    назад     5      обст   52123_9\n",
       "5       5     ЧМТ      ЧМТ     0      ROOT   52123_9\n",
       "...    ..     ...      ...   ...       ...       ...\n",
       "15255  14       в        в    17      обст   34695_0\n",
       "15256  15   армии    армия    14     предл   34695_0\n",
       "15257  16      не       не    17   огранич   34695_0\n",
       "15258  17  служил  служить     1     сочин   34695_0\n",
       "15259  19     ЧМТ      чмт    17  примыкат   34695_0\n",
       "\n",
       "[15259 rows x 6 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'medicalTextTrees/parus_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b95ea9bbdefe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrees_full_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrees_df_filtered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_3_sent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrees_df_filtered\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Time on reading the data: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-36f7ecb55eff>\u001b[0m in \u001b[0;36mread_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mDATA_PATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'medicalTextTrees/parus_results'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdf_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'form'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lemma'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'upostag'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'xpostag'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'feats'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'head'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprel'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# trees_df = pd.DataFrame(columns=df_columns)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'medicalTextTrees/parus_results'"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "trees_full_df, trees_df_filtered = read_data()\n",
    "test_3_sent = trees_df_filtered.head(12)\n",
    "print('Time on reading the data: ' + str(time.time() - start))\n",
    "\n",
    "part_of_speech_node_id = dict(trees_full_df[['lemma', 'upostag']].groupby(['lemma', 'upostag']).groups.keys())\n",
    "\n",
    "# get all lemmas and create a dictionary to map to numbers\n",
    "dict_lemmas = {lemma: [index] for index, lemma in enumerate(dict.fromkeys(trees_df_filtered['lemma'].to_list()), 1)}\n",
    "dict_lemmas_full = {lemma: [index] for index, lemma in\n",
    "                        enumerate(dict.fromkeys(trees_full_df['lemma'].to_list()), 1)}\n",
    "dict_lemmas_rev = {index[0]: lemma for lemma, index in dict_lemmas_full.items()}\n",
    "dict_lemmas_3 = {lemma: [index] for index, lemma in enumerate(dict.fromkeys(test_3_sent['lemma'].to_list()), 1)}\n",
    "    #\n",
    "numbers = [item for item in list(dict_lemmas_full.keys()) if pattern.match(item)]\n",
    "numbers_one_lemma = dict_lemmas_full[numbers[0]]\n",
    "for num in numbers:\n",
    "    dict_lemmas[num] = numbers_one_lemma\n",
    "    dict_lemmas_full[num] = numbers_one_lemma\n",
    "    if num in dict_lemmas_3.keys():\n",
    "        dict_lemmas_3[num] = numbers_one_lemma\n",
    "# get all relations and create a dictionary to map to numbers\n",
    "# dict_rel = {rel: index for index, rel in enumerate(dict.fromkeys(trees_full_df['deprel'].to_list()))}\n",
    "dict_rel = {rel: index for index, rel in enumerate(dict.fromkeys(trees_df_filtered['deprel'].to_list()))}\n",
    "train_word2vec(trees_full_df, dict_lemmas_full, dict_lemmas, part_of_speech_node_id)\n",
    "\n",
    "    #\n",
    "    # test_dict_lemmas = get_test_dict_lemmas()\n",
    "    # new_test_dict = {k: test_dict_lemmas[i] for i, k in enumerate(dict_lemmas_3.keys())}\n",
    "#     whole_tree = construct_tree(trees_df_filtered, dict_lemmas, dict_rel, dict_lemmas_rev)\n",
    "    # write_tree_in_table(whole_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f construct_tree construct_tree(trees_df_filtered, dict_lemmas, dict_rel, dict_lemmas_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print('Time on constructing the tree: ' + str(time.time() - start))\n",
    "    # whole_tree = new_test()\n",
    "    Tree.set_help_dict(whole_tree)\n",
    "    # partition nodes by height\n",
    "    start = time.time()\n",
    "    Tree.calculate_heights(whole_tree)\n",
    "    print('Time on calculating all heights: ' + str(time.time() - start))\n",
    "\n",
    "    heights_dictionary = {Tree.get_node(whole_tree, node_id): heights for node_id, heights in\n",
    "                          whole_tree.heights.items()}\n",
    "    grouped_heights = defaultdict(list)\n",
    "    for node_1, heights in heights_dictionary.items():\n",
    "        for height in heights:\n",
    "            grouped_heights[height].append(node_1)\n",
    "    grouped_heights = sorted(grouped_heights.items(), key=lambda x: x[0])\n",
    "\n",
    "    dict_lemmas_size = max(set(map(lambda x: x.lemma, whole_tree.nodes)))\n",
    "\n",
    "    # classes for partial repeats\n",
    "    start = time.time()\n",
    "    classes_part, classes_part_list = compute_part_new_new(whole_tree, dict_lemmas_size, grouped_heights)\n",
    "    write_tree_in_table(whole_tree)\n",
    "    # classes_part = compute_part_subtrees(whole_tree, dict_lemmas_size, grouped_heights)\n",
    "    print('Time on calculating partial repeats: ' + str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.29505109786987305\n",
      "1\n",
      "1.3842949867248535\n",
      "2\n",
      "1.6298727989196777\n",
      "3\n",
      "2.9113762378692627\n",
      "4\n",
      "3.745997190475464\n",
      "5\n",
      "4.040239095687866\n",
      "6\n",
      "0.9162311553955078\n",
      "7\n",
      "0.6441891193389893\n",
      "8\n",
      "0.6087419986724854\n",
      "9\n",
      "0.5605340003967285\n",
      "10\n",
      "0.5257279872894287\n",
      "11\n",
      "0.0004639625549316406\n",
      "12\n",
      "0.0005028247833251953\n",
      "13\n",
      "0.5428929328918457\n",
      "14\n",
      "0.5522897243499756\n",
      "15\n",
      "0.00029587745666503906\n",
      "Time on calculating partial repeats: 18.4059419631958\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#classes_part = compute_part_new_new(whole_tree, dict_lemmas_size, grouped_heights)\n",
    "%lprun -f compute_part_new_new compute_part_new_new(whole_tree, dict_lemmas_size, grouped_heights)\n",
    "print('Time on calculating partial repeats: ' + str((time.time() - start) % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
